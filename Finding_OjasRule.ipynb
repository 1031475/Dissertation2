{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import time\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the dataset: \n",
    "#### X = n_fd x n_sa x n_ni x n_da\n",
    "n_fd: number of parameters in the plasticity rule + 1  \n",
    "n_da: number of datasets presented to the pca network  \n",
    "n_sa: number of samples per dataset  \n",
    "n_ni: number of neurons in the pca network (= number of dimensions of the ambient space of the datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def generate_datasets_Gaussian(n_samples, n_datasets, n_finite_differences, D, dev):\n",
    "    #Xi taken from N(0,transp(Q)*D*Q)\n",
    "    #D diagonal, len(D) must be odd\n",
    "    \n",
    "    X = torch.zeros((n_samples, len(D), n_datasets), dtype=torch.double, device=dev)\n",
    "    for dataset_num in range(n_datasets):\n",
    "        A = np.random.rand(len(D),len(D))\n",
    "        Q, _ = np.linalg.qr(A)   \n",
    "        if np.linalg.det(Q) < 0:\n",
    "            Q = -Q\n",
    "        Cov = np.matmul(np.transpose(Q),np.matmul(D,Q)) \n",
    "        for sample_num in range(n_samples):\n",
    "            X[sample_num, :, dataset_num] = torch.tensor(np.random.multivariate_normal([0 for i in range(len(D))],Cov))\n",
    "    \n",
    "    #compute & store principal vectors to compare to PCA network performance afterwards\n",
    "    pc1 = np.zeros((len(D),n_datasets))\n",
    "    for d_num in range(n_datasets):\n",
    "        pca = PCA(n_components=2)\n",
    "        pca.fit(X[:,:,d_num].to('cpu').numpy())\n",
    "        pc1[:,d_num] = pca.components_[0]  \n",
    "    \n",
    "    return(X.repeat(n_finite_differences,1,1,1), torch.tensor(pc1, dtype=torch.double, device=dev))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial expansion of the plasticity rule\n",
    "#### Parallelized to update n_fd plasticity rules (compute all dimensions of the grad at once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def G(X,Y,W,A,eta,dev):\n",
    "    # parametrized plasticity rule\n",
    "    # eta = learning rate\n",
    "    # dev = cpu or gpu\n",
    "    # A coeffs of the n_fd plasticity rules considered\n",
    "    \n",
    "    DW = torch.zeros(X.size(),dtype=torch.double,device=dev)\n",
    "    ct = 0\n",
    "    for x in [torch.ones(X.size(),dtype=torch.double, device=dev), X, torch.mul(X,X)]: # (elementwise mult)\n",
    "        for y in [torch.ones(Y.size(),dtype=torch.double, device=dev), Y, torch.mul(Y,Y)]:\n",
    "            for w in [torch.ones(W.size(),dtype=torch.double, device=dev), W, torch.mul(W,W)]:\n",
    "                DW += torch.einsum(\"a,asnd,asd,and->asnd\",A[:,ct],x,y,w)\n",
    "                ct += 1\n",
    "    return(eta*DW/X.shape[1]) #divide by n_sa: independent of number of samples per dataset\n",
    "\n",
    "def G_1D(A, w, x, y):\n",
    "    #w, x, y scalars, returns a scalar\n",
    "    dw = 0\n",
    "    ct = 0\n",
    "    for pre in [1, x, x**2]:\n",
    "        for post in [1, y, y**2]:\n",
    "            for weight in [1, w, w**2]:\n",
    "                dw += A[ct]*pre*post*weight\n",
    "                ct += 1\n",
    "    return(dw)\n",
    "\n",
    "def G_oja(w, x, y):\n",
    "    return(x*y - w*(y**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA network:\n",
    "#### n_neurons (activity X) projecting on 1 neuron (activity y) with weights W.\n",
    "#### Parallelized on n_samples_per_datasets, n_datasets and n_fd (number of different plasticity rules run at the same time)\n",
    "W: n_fd x n_ni x n_da,   Y: n_fd x n_sa x n_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pca_net(): \n",
    "    \n",
    "    def __init__(self, n_epochs, n_datasets, n_samples_per_dataset, n_fd, n_neurons, a_blow, b_blow, dev):\n",
    "        self.n_sa = n_samples_per_dataset\n",
    "        self.n_ni = n_neurons\n",
    "        self.n_ep = n_epochs\n",
    "        self.n_da = n_datasets\n",
    "        self.n_fd = n_fd\n",
    "        self.a_blow = a_blow\n",
    "        self.b_blow = b_blow\n",
    "        \n",
    "        self.Y = torch.zeros((self.n_fd, self.n_sa, self.n_da),dtype=torch.double, device=dev)\n",
    "        self.W = torch.zeros((self.n_fd, self.n_ni, self.n_da),dtype=torch.double, device=dev)\n",
    "        \n",
    "        self.blow_up =  [self.n_ep for i in range(self.n_fd)] #epoch at which the nets blew up (n_ep = no blow up) \n",
    "        self.not_blown = [i for i in range(self.n_fd)] #dimensions which did not blow up yet\n",
    "        \n",
    "    def forward(self, X):\n",
    "        self.Y = torch.einsum(\"asnd,and->asd\", X, self.W)\n",
    "    \n",
    "    def train(self, A_full, eta, X, pc1, dev):\n",
    "        \n",
    "        ###### W initilisation, choose between 3 options ######\n",
    "        aux = 0.1*torch.randn([self.n_ni, self.n_da], dtype=torch.double, device=dev); self.W = aux.repeat(self.n_fd,1,1)\n",
    "#         self.W = torch.load(\"Winit1.pt\"); self.W = self.W[0, :, :].repeat(self.n_fd, 1, 1);\n",
    "#         self.W = pc1.clone().detach().repeat(self.n_fd,1,1) #initialise W at the solution\n",
    "        #######################################################\n",
    "        \n",
    "        for epoch_num in range(self.n_ep):\n",
    "            self.forward(X)\n",
    "            self.W += torch.sum(G(X, self.Y, self.W, A_full, eta, dev), 1)\n",
    "            \n",
    "            ###### handle blow-ups\n",
    "            if not torch.equal(self.W,self.W): #nan != nan: there is a nan somewhere (ie blow up). Fast to compute\n",
    "                dim_to_remove = []\n",
    "                for fd_num in self.not_blown:\n",
    "                    if not torch.equal(self.W[fd_num, :, :], self.W[fd_num, :, :]):\n",
    "                        print(\"epoch_num =\" + str(epoch_num) + \";  blow_up on dim \" + str(fd_num))\n",
    "                        self.blow_up[fd_num] = epoch_num\n",
    "                        dim_to_remove.append(fd_num)\n",
    "                self.not_blown = [x for x in self.not_blown if x not in dim_to_remove]\n",
    "                \n",
    "        for fd_num in self.not_blown: #check for \"almost blow ups\" at the end of training (almost nan), which can screw up the grad\n",
    "            dim_to_remove = []\n",
    "            if torch.max(self.W[fd_num]).item() > 100000/self.n_sa:\n",
    "                print(\"epoch_num =\" + str(self.n_ep) + \";  blow_up (to come) on dim \" + str(fd_num))\n",
    "                self.blow_up[fd_num] = self.n_ep - 1\n",
    "                dim_to_remove.append(fd_num)\n",
    "        self.not_blown = [x for x in self.not_blown if x not in dim_to_remove]\n",
    "        \n",
    "    def score(self, pc1):\n",
    "        has_blown_up = False # has any fd perturbations of the network blown up during training\n",
    "        err_mean_pca = [0]*self.n_fd\n",
    "        for fd_num in range(self.n_fd):\n",
    "            if self.blow_up[fd_num] < self.n_ep: # blow up on this perturbation of A\n",
    "                has_blown_up = True\n",
    "                err_mean_pca[fd_num] = self.a_blow*(self.n_ep - self.blow_up[fd_num])/(self.n_ep) + self.b_blow\n",
    "            else:\n",
    "                ##############################################\n",
    "                aux = self.n_da*(self.n_ni**(1/3))/(3**(1/3))\n",
    "                for d_num in range(self.n_da):\n",
    "                    err_mean_pca[fd_num] += (min(torch.norm(pc1[:, d_num] - self.W[fd_num, :, d_num]).item(), torch.norm(pc1[:, d_num] + self.W[fd_num, :, d_num]).item()))/aux\n",
    "        return(has_blown_up, err_mean_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent on the plasticity rule\n",
    "Gradient computed with finite differences, ADAM optimizer (modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class learningG_finite_diff():\n",
    "       \n",
    "    def __init__(self, D, n_samples_per_dataset, n_epochs, n_datasets, A, eta, h, meta_eta, beta1, beta2, epsilon, reg, a_blow, b_blow, n_it_back):\n",
    "        self.n_sa = n_samples_per_dataset\n",
    "        self.n_ni = len(D)\n",
    "        self.n_ep = n_epochs\n",
    "        self.n_da = n_datasets\n",
    "        self.n_fd = 2*len(A) + 1\n",
    "        \n",
    "        ###### Generation of the training datasets ######\n",
    "        self.X,self.pc1 = generate_datasets_Gaussian(self.n_sa, self.n_da, self.n_fd, D, dev)\n",
    "#         self.X = torch.load(\"X1.pt\"); self.pc1 = torch.load(\"pcX1.pt\"); self.X = self.X[0, :, :, :].repeat(self.n_fd, 1, 1, 1)\n",
    "        ##################################################\n",
    "        \n",
    "        self.A = A.clone().detach()\n",
    "        self.eta = eta\n",
    "        self.m_eta = meta_eta\n",
    "        self.h = h\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.current_meta_it = 0\n",
    "        self.m = torch.zeros(len(self.A), dtype=torch.double, device=dev)\n",
    "        self.v = torch.zeros(len(self.A), dtype=torch.double, device=dev)\n",
    "        self.reg = reg\n",
    "        self.a_blow = a_blow\n",
    "        self.b_blow = b_blow\n",
    "        self.last_blow_up = 0\n",
    "        self.n_it_back = n_it_back\n",
    "        \n",
    "        # Generating the perturbations on each dim of A for finite differences method: calc on both sides\n",
    "        self.A_h = torch.zeros(self.n_fd, len(A), dtype=torch.double, device=dev)\n",
    "        for dim_num in range(len(A)):\n",
    "            self.A_h[2*dim_num + 1, dim_num] = h\n",
    "            self.A_h[2*dim_num + 2, dim_num] = (-h)\n",
    "        \n",
    "        # for plotting\n",
    "        self.A_hist = self.A.clone().detach()\n",
    "        self.loss_hist = []\n",
    "        self.angle_hist = []\n",
    "    \n",
    "    def get_grad(self, dev):\n",
    "        grad = torch.zeros(self.A.shape, dtype=torch.double)\n",
    "        net = pca_net(self.n_ep, self.n_da, self.n_sa, self.n_fd, self.n_ni, self.a_blow, self.b_blow, dev)\n",
    "        A_full = self.A + self.A_h\n",
    "        has_blown_up ,loss_no_L1, losses = self.loss(A_full, net)\n",
    "#         print(\"losses \" + str(losses))\n",
    "        g = torch.tensor(([(losses[2*i + 1] - losses[2*i + 2])/(2*self.h) for i in range(len(self.A))]), dtype=torch.double, device=dev)\n",
    "#         print(\"grad \" + str(g))\n",
    "        return(has_blown_up, loss_no_L1[0], g)\n",
    "    \n",
    "    def loss(self, A_full, net):\n",
    "        net.train(A_full, self.eta, self.X, self.pc1, dev)\n",
    "        l1 = np.multiply([torch.norm(A_full[i, :],p = 1).item() for i in range(self.n_fd)],self.reg)\n",
    "        has_blown_up, loss_no_l1 = net.score(self.pc1)\n",
    "        return(has_blown_up, loss_no_l1, loss_no_l1 + l1)\n",
    "\n",
    "    def train(self, A_oja, dev, n_meta_it): #ADAM optimizer with momentum reset\n",
    "        initial_meta_it = self.current_meta_it\n",
    "        y = A_oja.cpu().numpy() # for angle calculation after\n",
    "        \n",
    "        while self.current_meta_it < (initial_meta_it + n_meta_it):   \n",
    "            has_blown_up, loss, g = self.get_grad(dev)\n",
    "#             print(\"current loss no l1 \" + str(loss))\n",
    "            \n",
    "            if ((not has_blown_up) or (has_blown_up and (self.current_meta_it - self.last_blow_up < 11))):\n",
    "                # no momentum reset if no blow up at this meta_it or blow up following a too recent blow up \n",
    "                self.m = self.beta1*self.m + (1-self.beta1)*g #momentum\n",
    "                self.v = self.beta2*self.v + (1-self.beta2)*torch.mul(g,g) #2nd moment estimate\n",
    "\n",
    "            else:\n",
    "                # reset momentum/second moment estimate and restart n_it_back meta_it before\n",
    "                self.current_meta_it -= self.n_it_back\n",
    "                self.last_blow_up = self.current_meta_it\n",
    "                self.A_hist = self.A_hist[0: (self.A_hist.size()[0] - 27*self.n_it_back)]\n",
    "                self.A = self.A_hist[-27:].clone().detach()\n",
    "                self.loss_hist = self.loss_hist[0: (len(self.loss_hist) - self.n_it_back)]\n",
    "                self.angle_hist = self.angle_hist[0: (len(self.angle_hist) - self.n_it_back)]\n",
    "                print(\"momentum reset at meta_it \" + np.str(self.current_meta_it))\n",
    "                \n",
    "                has_blown_up, loss, g = self.get_grad(dev)\n",
    "                self.m = (1-self.beta1)*g\n",
    "                self.v = (1-self.beta2)*torch.mul(g,g)\n",
    "                   \n",
    "            ###### bias correction or not ######\n",
    "#             m_hat = self.m/(1-(self.beta1**(self.current_meta_it+1)))\n",
    "#             v_hat = self.v/(1-(self.beta2**(self.current_meta_it+1)))\n",
    "            m_hat = self.m\n",
    "            v_hat = self.v\n",
    "#             print(\"m \" + str(m_hat))\n",
    "#             print(\"v \" + str(v_hat))\n",
    "            \n",
    "            self.A -= self.m_eta*torch.div(m_hat,torch.sqrt(v_hat) + self.epsilon) #update A\n",
    "            \n",
    "            if not torch.equal(self.A,self.A):\n",
    "                print(\"a nan made it inside A\")\n",
    "                return()            \n",
    "            self.A_hist = torch.cat((self.A_hist, self.A),0)\n",
    "            self.loss_hist.append(loss)\n",
    "            x = self.A.cpu().numpy();\n",
    "            self.angle_hist.append(np.arccos(np.dot(x, y)/((np.linalg.norm(x)*np.linalg.norm(y))))*180/(np.pi))\n",
    "            if (self.current_meta_it % 10 == 0):\n",
    "                print(\"iteration \" + str(self.current_meta_it+1) + \"/\" + str(initial_meta_it + n_meta_it))\n",
    "                print(\"current_loss (without L1 term): \" + str(loss))  \n",
    "            self.current_meta_it += 1\n",
    "\n",
    "    def plot(self):\n",
    "        plt.figure(1)\n",
    "        for dim_num in range(len(self.A)):\n",
    "            plt.plot([self.A_hist[dim_num + len(self.A)*i].item() for i in range(self.current_meta_it)], linewidth = 8)\n",
    "        plt.title(\"evolution of coefficients of A through training\")\n",
    "        plt.show()\n",
    "        \n",
    "        plt.figure(2)\n",
    "        plt.plot(self.loss_hist, linewidth = 4)\n",
    "        plt.title(\"loss across iterations\")\n",
    "        plt.show()\n",
    "        \n",
    "        plt.figure(3)\n",
    "        plt.plot(self.angle_hist, linewidth = 4)\n",
    "        plt.title(\"angle A - A_oja across iterations\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation\n",
    "\n",
    "#### 1/ parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "###### Synthetic dataset parameters, D has to be odd dimension\n",
    "D3 = np.diag([2,1,0]); D3bis = np.diag([1,0.5,0])\n",
    "D5 = np.diag([2,1,1,0,0]); D5bis = np.diag([1,0.7,0.2,0.1,0])\n",
    "D11 = np.diag([5,4,3,2,1,0,0,0,0,0,0]); D11bis = np.diag([1,0.8,0.5,0.3,0.1,0,0,0,0,0,0])\n",
    "D39 = np.diag([10,9,9,9,8,8,8,7,7,7,5,5,5,5,5,5,5,5,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0])\n",
    "D39bis = np.diag([1,0.9,0.9,0.9,0.8,0.8,0.8,0.7,0.7,0.7,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0,0,0,0,0,0,0,0,0,0,0])\n",
    "\n",
    "###### PCA_net parameters\n",
    "n_samples_per_dataset = 100; n_epochs = 200; eta = 1/20\n",
    "\n",
    "###### Meta optimisation parameters\n",
    "n_meta_it = 10; n_datasets = 100; h = 1/200; meta_eta = 1/200; beta1 = 0.9; beta2 = 0.999; \n",
    "epsilon = 1e-6; reg = 0.1; a_blow = 1; b_blow = 2; n_it_back = 2\n",
    "\n",
    "###### Initialisation\n",
    "A_oja = torch.tensor([0,0,0,0,0,0,0,(-1),0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0], dtype=torch.double, device=dev)\n",
    "A_0 = torch.tensor([0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0], dtype=torch.double, device=dev)\n",
    "A_close_oja    = torch.tensor([-0.0810, -0.0161, -0.1892, -0.1068,  0.0860,  0.0048,  0.0403, -1.1123,\n",
    "         0.0957, -0.0231, -0.0791,  0.1590,  1.0008,  0.1013, -0.2789,  0.0728,\n",
    "         0.0148,  0.2366, -0.1421,  0.0984, -0.0901,  0.0740,  0.1227,  0.0633,\n",
    "        -0.0386, -0.0694, -0.0459], dtype=torch.double, device=dev)\n",
    "A_rand_0 = torch.tensor([ 0.0867,  0.0778, -0.0047, -0.1911, -0.1838, -0.0798, -0.0325,  0.0380,\n",
    "         0.0030,  0.0540, -0.0596,  0.0153, -0.1660, -0.0241,  0.1270, -0.0591,\n",
    "        -0.0492, -0.0512, -0.0270, -0.0016, -0.0454,  0.0275,  0.1039, -0.0373,\n",
    "        -0.0933, -0.1817,  0.0762], dtype=torch.double, device=dev)\n",
    "A_unstable = torch.tensor([ 0.1945, -0.4187,  1.3444, -0.0039,  0.2078, -0.9590,  0.7205,  1.3984,\n",
    "        -0.4893, -0.9633, -0.7834,  0.0169, -1.0988, -1.6190, -1.1393, -0.8860,\n",
    "         0.9665, -0.9679, -0.1661, -0.4697, -0.3577, -0.6227, -0.1183,  0.4971,\n",
    "        -0.2486, -1.8236,  0.2660], dtype=torch.double, device=dev)\n",
    "A_adam1 = torch.tensor([ 0.0102, -0.0200, -0.0091, -0.0054, -0.0050, -0.0050,  0.1968, -0.3316,\n",
    "        -0.0042, -0.0057, -0.0037, -0.0048,  0.2370, -0.1286, -0.0873,  0.0192,\n",
    "        -0.0199,  0.0208, -0.0545, -0.2052, -0.0135, -0.0332,  0.0178, -0.0219,\n",
    "        -0.1949, -0.8068, -0.3349], dtype=torch.double, device=dev)\n",
    "A_adam2 = torch.tensor([-5.5102e-02, -1.4344e-01,  9.0817e-03, -1.0121e-02, -2.4525e-03,\n",
    "        -5.3187e-03,  1.3737e-01, -2.7457e-01,  2.6577e-02, -6.3599e-03,\n",
    "        -1.6456e-04, -4.2544e-03,  5.1275e-01, -1.9923e-02, -2.3343e-02,\n",
    "        -7.9909e-03, -4.7050e-03, -3.3497e-03, -9.8408e-02,  3.6230e-02,\n",
    "        -2.2974e-03, -1.1924e-02, -7.9329e-03, -7.6932e-03, -6.9461e-02,\n",
    "        -2.5508e-01, -1.5929e-01], dtype=torch.double, device=dev) #A_adam2 is iteration 120 from plast_rule17\n",
    "A_rand = 0.01*torch.randn(A_oja.size(),dtype=torch.double, device=dev)\n",
    "Atest = torch.tensor([ 0.0206, -0.0080,  0.0019, -0.0092,  0.0112,  0.0097,  0.0056, -0.4870,\n",
    "        -0.0165, -0.0193, -0.0083,  0.0021,  0.6872,  0.0121,  0.0276, -0.0076,\n",
    "        -0.0091, -0.0112, -0.0181,  0.0651,  0.0073, -0.0084, -0.0043, -0.0025,\n",
    "        -0.0166, -0.1224, -0.0085], device='cuda:0', dtype=torch.float64)\n",
    "Atest2 = torch.tensor([-4.6946e-02, -2.4606e-02,  7.6413e-03,  1.3415e-02, -3.9182e-04,\n",
    "         5.4507e-05,  2.7690e-02, -4.9433e-01,  9.2513e-02,  1.8933e-02,\n",
    "         7.0540e-03,  8.6338e-04,  5.9564e-01,  8.2658e-03,  3.2073e-02,\n",
    "         1.3348e-02, -2.8721e-03,  4.3265e-04, -3.3670e-02,  6.5483e-02,\n",
    "         6.8184e-03,  2.5619e-03, -4.4377e-03, -1.2937e-03,  1.6686e-02,\n",
    "        -7.9009e-02, -8.7798e-02], device='cuda:0', dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2/ simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time in s: 0.0s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "###### START A NEW SIMULATION #######################\n",
    "plast_rule = learningG_finite_diff(D, n_samples_per_dataset, n_epochs, n_datasets, A_rand_0, eta, h, meta_eta, beta1, beta2, epsilon, reg, a_blow, b_blow, n_it_back)\n",
    "print(\"datasets generated (\" + str(np.round(time.time() - start,2)) + \"s)\")\n",
    "#####################################################\n",
    "\n",
    "###### CONTINUE WORKING ON A PRE_TRAINED MODEL ######\n",
    "# plast_rule = torch.load(\"rule34.pt\")\n",
    "# plast_rule.plot()\n",
    "#####################################################\n",
    "\n",
    "plast_rule.train(A_oja, dev, n_meta_it)\n",
    "plast_rule.plot()\n",
    "\n",
    "print(\"Total time in s: \" + str(np.round(time.time() - start,2)) + \"s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
